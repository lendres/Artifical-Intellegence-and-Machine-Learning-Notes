	\chapter{Linear Regression}
Residuals are the distance from the data point to the fitted line or linear regression.

Mean square of errors
	\begin{equation}
		m_e = \frac{\sum \left(y_i - y_p\right)^2}{n-2} = \frac{\sum e_i^2}{n-2}
	\end{equation}

Standard deviation of the residuals, also called the root mean square of the errors.

	\begin{equation}
		s_e = \sqrt{\frac{\sum \left(y_i - y_p\right)^2}{n-2}} = \sqrt{\frac{\sum e_i^2}{n-2}} = \sqrt{m_e}
	\end{equation}

Mean absolute error
	\begin{equation}
		a_e = \frac{\sum \left|y_i - y_p\right|}{n-2} = \frac{\sum \left|e_i\right|}{n-2}
	\end{equation}

	\begin{mathwhere}
		\mathdefitem{y_i}{actual value;}
		\mathdefitem{y_p}{predicted value;}
		\mathdefitem{e}{residual;}
		\mathdefitem{n}{number of observations/samples.}
	\end{mathwhere}

Coefficient of determination, often call the R squared score, is the proportion of the variation in the dependent variable that is predictable from the independent variable(s).  A score of 1 means the variance is perfectly captured.  

As more parameters are added to the model, the R squared score will increase.  Even if the parameters does not correlate well with the output, by chance it can improve the result.

Adjusted R squared is a modification to R squared to create a more intuitive response to adding model parameters.  If the added parameter improves the response, the adjusted R square value goes up.  If it does not add \emph{enough} value to the response, the adjusted R square value falls.

	\section{Over Fit and Under Fit}
An over fit model will capture noise as well as the information.  An under fit model will not capture the available information.

	\subsection{R Square Score}
If the model is over fit, the R squared score on the training data and the testing data will not match well. 